\chapter{Review of Literature: Algorithms, Models, and Metric Formulas} \label{chp:review-of-literature}

%\section{Algorithms, Models, and Metric Formulas}
%\section{A Brief Background of Consumption Large Data}

\section{Introducing Query Search and Recommender Systems}
In our information-based society, consumers of text, such as researchers want to derive useful data or make meaningful connections from textual corpora. This is a problem of \textit{Information Retrieval} (IR). Some IR applications are \textit{query search} (QS), \textit{recommendation (or recommender) systems} (RS), and summarization \citep{Mani01summarizationevaluation:2001}.
%Search queries are common and many users are familiar with their use.
These IR approaches are not without drawbacks. QS, for example, often requires that a user have at least some knowledge of the corpus being queried. This includes some knowledge of pertinent vocabulary for the corpus being queried. For people who are new to a body of data, (e.g. new church lay members accessing the \textit{CLDSGCT}), this is not an entirely reasonable expectation. For example, upon submitting a search query, a person might be able to locate a handful documents of interest, but finding more can quickly become an increasingly difficult process of sifting through lower and lower-ranked search results, unless the user expands the search by trying a variety of related vocabulary. \textit{If you have ever had to submit a second or third query to Google search to find what you wanted, you may be a first-hand witness to a problem that even Google has yet to solve.} A use-case/scenario demonstrating QS on our data domain is included in Appendix ~\ref{scenario}. % TODO: make sure this works.

When users lack the required vocabulary, or already know of 1 document they like, recommender systems can become helpful. Given some starting document, a recommender system can find documents that are characterized as recommended/similar. When this is done on-the-fly, it is an \textit{online} algorithm; pre-computed, \textit{offline} algorithm.

Query search can be seen as a type of recommender system which uses the query itself as a document to match against all other documents. The similarities between QS and RS don't stop at--they extend to some of the algorithms, metrics, and evaluation techniques used on them. For that reason, both QS and RS will be described in this chapter. % TODO: Be less...theoretical.

% TODO: Put this somewhere? `Presenting recommendations in this way is a use of the long-tail phenomenon.` Maybe in an affilial chapter or as part of methodology.

Recommender systems typically involve processing phases: first, measurable features are discovered and assigned to items in the dataset and second, an algorithm matches items of similar features. When available, personalized data such as ratings may be leveraged to provide personalized recommendations.

Despite their differences, QS and RS can use similar algorithms. While many algorithms have been implemented in QS and RS, I will focus on the following in this chapter: TF-IDF in search and recommendation, LDA in recommendation, k-NN, (Collapsed) Gibbs Sampling, Variational Inference, and `LDA-inspired' algorithms with provable guarantees.

\section{Algorithms}
\subsection{TF-IDF: An Algorithmic Model}
The first major model-algorithm is TF-IDF, which is short for term frequencyâ€“inverse document frequency. It is a bag-of-words model of term weights stored in matrices. For programmers, it is a readily available model because it is well-known, easy to understand, and included in various open-source products such as Lucene\textregistered \citep{McCandless:2010:LAS:1893016}, Solr\textregistered \citep{apache_solr_beginners_guide_2013,apache_solr_enterprise_search_server_2015}, NaturalNode's natural \citep{github-NaturalNode-natural}, and Elasticsearch\textregistered \citep{es-site}. This means that for systems such as the Scripture Citation Index, both systems can share a code base, simplifying the overall code\footnote{Interestingly, for software engineers this sharing of code can be considered `elegant' just as theories in physics can be considered `elegant'. \textit{Lex parsimoniae}, or Occam's Razor, is an attractive axiom in both physics and software engineering--and therefore in computational linguistics as well.}. Speaking from personal experience, this is a desirable characteristic for time/money-constrained engineering projects, especially those that strive for simplicity of maintenance.

Karen Sp\"{a}rck Jones wrote the seminal paper on TF-IDF in \citeyear{sparck1972statistical} \citep{sparck1972statistical}. 
%It is highly cited. Despite its age, the paper continues to be highly cited and was republished as late as 2004
Sp\"{a}rck's work inspired other computer scientists such as Stephen Robertson to conduct research in TF-IDF, which has been used frequently in the field of query search. Sp\"{a}rck Jones worked with Stephen Robertson to create Okapi BM25 and later improved versions, a set of functions used in document retrieval that uses TF-IDF principles. Later, Stephen Robertson worked to develop the Bing search engine. Open-source tools such as Lucene \citep{McCandless:2010:LAS:1893016}, Lucene Luke \citep{lucene:luke}, AntConc \citep{anthony_2013} have been created as a result of widespread usage of TF-IDF. LDS SCI uses Lucene Luke, and LDS.org search results are typically so similar to LDS SCI that it appears to also use TF-IDF behind-the-scenes (or something that produces the same frequency-based results).

Building on the raw frequency concept to compute the TF scores of the TF-IDF model, \citet{manning_raghavan_2008_scoring} mentions three other ways to compute, namely: (1) Boolean ``frequencies,'' (2) logarithmic scale, and (3) augmented frequency. These all affect the algorithm by affecting the built-in bias. They are approaches to the problem of how to avoid giving excess weight to terms found in longer documents. Long documents have a better chance of having key words more frequently, but more words doesn't always correspond with additional information.  Similarly, the IDF portion can be calculated in a variety of ways. Variations of IDF, such as smoothing or a probabilistic approach affect the bias of how important any given word is in a corpora. All these variants can pose a challenge for researchers as they must determine the correct biases for the specific project. Another notable way to compute document similarity for search is to use word correlation factors \citep{won2007using}.

TF-IDF weighting is inspired by Zipf's law, which states that some words are used many times while the majority of words will occur less often. The weight, or value, of a word is in inversely proportional to the number of occurrences, hence IDF, inverse document frequency. On this Sp\"{a}rck stated, ``The appropriate way of doing [weighting] is suggested by the term distribution curve for the vocabulary, which has the familiar Zipf shape.'' \citeauthor{Wu:2008:Interpreting_tf_idf_term_weights} similarly said ``The term-frequency factor was originally thought to be indicative of document topic [Luhn 1958], and the inverse document-frequency (IDF) is reasoned [Sp\"arck Jones 1972] on the basis of Zipf law. \citeyearpar{Wu:2008:Interpreting_tf_idf_term_weights}'' It is important to note that not all IDF score metrics attempt to respect Zipf's law. One such example of this is the unary IDF scoring, which simply assigns a value of 1 to all IDF scores.

TF-IDF can also be used in the creation of a recommender system with all the same bias adjustments mentioned previously. Each document has a corresponding TF-IDF vector which is a structured abstraction. These structures can be compared. Researchers can employ a variety of algorithms to compare the vectors to make meaningful connections. %For example, \citeauthor{Meteren_usingcontent_based} used kNN with a cosine metric and published their findings \citep{Meteren_usingcontent_based}. k-NN will be discussed in further detail later in this literature review.
Where more than 1 textual metadatum exists for each document, each metadatum can be considered separately, then weighted appropriately as a zone \citep{manning_raghavan_2008_scoring}. Examples of this would be textual titles and textual content. Although they are separate, they may both be used for matching purposes. \citeauthor{Wu:2008:Interpreting_tf_idf_term_weights} provide a novel use of TF-IDF by creating both local relevance that ``only applies to a specific document location, and [non-local], common, type is the `document-wide' relevance that applies to the entire document. The model combines the local relevance for every location of a document by the document-wide relevance decision of the document. \citeyearpar{Wu:2008:Interpreting_tf_idf_term_weights}'' So even as late as 2008 TF-IDF continues to be developed!

Overall, TF-IDF is considered to be a powerful tool for information retrieval by experts in the field. Robertson \citeyearpar{understanding_idf_2004} said, ``The class of weighting schemes known generically as TF*IDF\footnote{Note the asterisk in place of the hyphen.}, which involve multiplying the IDF measure (possibly one of a number of variants) by a TF measure (again possibly one of a number of variants, not just the raw count) have proved extraordinarily robust and difficult to beat, even by much more carefully worked out models and theories.'' A notable example includes Netflix's use of context \citep{Bell:2007:lessons_from_the_netflix_prize}.

% TODO: Wrap-up with how TF-IDF can be used to produce recommendations.

\subsection{LDA: A Model and Algorithms to Build It}
Other researchers have continued to work on ways to improve text corpora modeling. A significant model was introduced by \citeauthor{Blei:2003:LDA:944919.944937} called Latent Dirichlet Allocation (LDA) in their paper published in \citeyear{Blei:2003:LDA:944919.944937} \citep{Blei:2003:LDA:944919.944937}. They assert that LDA can overcome weaknesses in previous models, stating that TF-IDF ``provides a relatively small amount of reduction in description length and reveals little in the way of inter- or intra-document statistical structure.'' Pre-dating LDA, the models used by researched were Latent Semantic Indexing, LSI, and later Probabilistic Latent Semantic Indexing, or pLSI. \citeauthor{Blei:2003:LDA:944919.944937} explain that ``pLSI is incomplete in that it provides no probabilistic model at the level of documents. In pLSI, each document is represented as a list of numbers (the mixing proportions for topics), and there is no generative probabilistic model for these numbers. This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting, and (2) it is not clear how to assign probability to a document outside of the training set.'' LDA is a generative probabilistic model, so in these respects, it is an improvement over pLSI \citeyearpar{Blei:2003:LDA:944919.944937}.

LDA has been used by many researchers for topic modeling, it has also been applied to word-sense disambiguation \citep{boyd2007topic}, DNA research \citep{Pritchard945, huelsenbeck2006dirichlet, shivashankar2011multi}, and query search \cite{Wei:2006:LDM:1148170.1148204}. This means that LDA, like TF-IDF, is both popular and flexible for information retrieval tasks.
 
LDA hinges squarely on intuitions in Bayesian statistics which, to quote \cite{jeffreys1973scientific} ``[Bayes' theorem] is to the theory of probability what the Pythagorean theorem is to geometry.'In the LDA model, the observable variable of words is used to discover the latent (hidden) variables of topics in the corpora using Bayesian statistics using a Dirichlet. ``the [Dirichlet probability distribution] is a convenient distribution on the simplex -- it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution \citep{jeffreys1973scientific}.'' Because it is a bag-of-words approach, the position of words are interchangeable. Therefore, two documents with the same words, but in different order, are topic-equivalent. However, the LDA model for each may differ, depending on the algorithm used to infer the model.
%TODO: [Paul help here!!]


% TODO: Mention that LDA, by using a dirichlet prior, avoids overfitting. https://www.reddit.com/r/MachineLearning/comments/10mdtf/lsa_vs_plsa_vs_lda/

In the process of deriving the LDA model, an assumption is made to allow the formula to be simplified. The simplification, as it applies to text, is the assumption that topics do not influence each other. This is a core weakness to the basic LDA model. Nevertheless, it does well at tagging words as belonging to particular topics. Of course, for some applications, ignoring this assumption will not be appropriate. \citeauthor{arora2013practical} put it this way: ``Although standard topic models like LDA \citep{blei2003latent} assume that topic proportions in a document are uncorrelated, there is strong evidence that topics are dependent \citep{blei2007correlated, li2006pachinko}: economics and politics are more likely to co-occur than economics and cooking. \citeyearpar{arora2013practical}'' So even Blei, who introduced LDA, admits that it has shortcomings! %TODO: [Paul help here!!]

The parameters of the LDA model can be inferred (i.e. computed) by using Variational Inference, Variational Bayes, collapsed Variational Bayes \citep{teh2006collapsed, blei2006variational, NIPS2010_3902}, Gibbs Sampling, or Collapsed Gibbs Sampling \citep{Porteous:2008:FCG:1401890.1401960}. % TODO: add comment here
%TODO: [Paul help here!!]

Collapsed Gibbs Sampling for LDA happens to be implemented in the Mallet toolkit \citep{McCallumMALLET}, an open-source software package. This means that for software engineers that are not interested in writing their own version of the algorithm can just use the open-source code. % TODO: mention that the author of this work used it previously, and link to some articles (need to post those online!)

% TODO: mention other types of LDA, such as hierarchical \citep{missing}...will need to explain why we didn't use those.

% TODO: Wrap-up with how LDA can be used to produce recommendations in 1 sentence!

% TODO: Add figures here
%See Figure xyz for the plate notation of the LDA model.

%See Figure xyz for the (non-plate) expanded form of the LDA model.

%INSERT FIGURE I (plate explanation).

%INSERT FIGURE I (non-plate explanation).

%See Figure xyz for a formula for Gibbs Sampling.

%See Figure xyz for a formula for LDA.

\subsubsection{Beyond LDA}
%TODO: replace \citeyearpar with \cite once it is known
LDA is not the only modern topic model. Research in \citeyear{arora2012learning} started to indicate that some methods of inferring topic models can have provable guarantees \citep{arora2012learning}, which is an improvement over LDA. One algorithm was presented by \cite{arora2012learning} which uses anchor words and assumes the topics are correlated. In contrast, \cite{anandkumar2012spectral} present an algorithm which assumes topics are not correlated. To quote \cite{arora2012learning}, ``Both algorithms run in polynomial time, but the bounds that have been proven on their sample complexity are weak and their empirical runtime performance is slow. It is also unclear how they perform if the data does not satisfy the modeling assumptions'' \citep{arora2013practical}. In this same article, \citeauthor{arora2013practical} present an algorithm with provable guarantees that is also practical (it doesn't violate model assumptions).

\begin{displayquote}
``Our algorithm performs as well as collapsed Gibbs sampling on a variety of metrics, and runs at least \textit{an order of magnitude faster}, and as much as fifty times faster on large datasets, allowing real-time analysis of large data streams. Our algorithm inherits the \textit{provable guarantees} of [previous algorithm] and results in simple, \textit{practical implementations}. We view this work as combining the best of two approaches to machine learning: the tractability of statistical recovery with the robustness of maximum likelihood estimation....

[These new] algorithms for topic modeling...are efficient and simple to implement yet maintain provable guarantees. The running time of these algorithms is effectively independent of the size of the corpus. Empirical results suggest that the sample complexity of these algorithms is somewhat greater than MCMC, but, particularly for the \textit{l\textsubscript{2}} variant, they provide comparable results in a fraction of the time. We have tried to use the output of our algorithms as initialization for further optimization (e.g. using MCMC) but have not yet found a hybrid that out-performs either method by itself. Finally, although we defer parallel implementations to future work, these algorithms are parallelizable, potentially supporting web-scale topic inference.'' (\citealp{arora2013practical}; emphasis added)
\end{displayquote}

This means that they can be even faster! Whether or not the algorithms introduced by \citeauthor{arora2013practical} lend themselves to recommender systems is not mentioned in this article.
% previous l-cursive sub 2 was `\ell $_{\text{2}}$`

\section{Using the Models to Compute Recommendations}
% TODO: fill in the blank or just remove it.
The TF-IDF and LDA models alone are not recommender models. However, they \textit{are} simplified, abstracted, structured models of unstructured data. Although not recommender models themselves, both lend themselves to use in recommender systems, given that metrics are used to compute distances/similarities between documents. For the LDA model, distribution over topics are used as points of comparison; for TF-IDF, the vector of token counts are the compared objects. By using a distance metric, recommendations can be calculated. % Research which have done this include: \citep{paik2010recommendation,missing,missing}.  % TODO: Some of the algorithms they used include: k-NN, \_\_\_, \_\_\_.

Since TF-IDF and LDA are different model types rather than instantiations of the same model \footnote{All things being equal (e.g. normalization and other settings), many TF*IDF variants would produce different models within the same space.}. Their data is in different spaces. Although both LDA and TF-IDF contain a second order tensor \citep{turney2010frequency}, or matrix, each vector of the matrix is in a different space. In TF-IDF, the space is lower-bounded by 0, having an upper-bound which is corpus-dependent. This space can be described approximately with the following rules, some of which are stated explicitly to contrast with those of LDA:

\begin{enumerate}
    \item The sum over the scalars in any vector does not need to sum to 1.
    \item The sum over the scalars in any vector should always be positive.
    \item The sum over the scalars should generally be non-zero. (A sum of 0 indicates all the words of the document were stop-words or the document was mis-retrieved or removed as sometimes happens with documents posted on the internet.)
\end{enumerate}

For LDA, the vectors are in the probability simplex \citep{blei2003latent}, or \textit{T}-space. The space can be approximately described with the following rules:

\begin{enumerate}
    \item All scalars in any vector are positive.
    \item All scalars in a vector lie within the range of [0, 1].
    \item The sum over these scalars equals 1 for any vector.
\end{enumerate}

%TODO: fact check and cite (Jensen-Shannon vs. Hellinger may have been switched up here!
Since TF-IDF and LDA models are in mathematically different spaces, it is not surprising that the similarity metrics used on them differ as well. For TF-IDF's vector-frequency space, a cosine similarity metric is appropriate. In LDA's space, the Hellinger or Jensen-Shannon metrics are appropriate. \cite{Krstovski2013efficient} showed that a simplification of the Jensen-Shannon may be used to increase the speed of k-NN without any negative impact. Their speed-up algorithm takes advantage of the realization that the square rooting portion of the distance formula is not necessary to compute relative distance. (It would be required for computing the \textit{actual} distance between documents. For recommender systems which do not expose similarities as distances, the two metrics are functionally equivalent, with Hellinger being faster \cite{Krstovski2013efficient}.) Similarly, a speed-up metric can also be employed for TF-IDF's cosine metric by removing the square root from its computation, again with the effect that exact distances are not compute. %As cosine distance is often employed in k-NN algorithms, the application of such a speed-up is assumed as a given\footnote{i.e. it is obvious simplification}

\section{Evaluation}
Once an RS is built and recommendations are made, it is good practice to compare the results to either a gold standard or to the results of another RS. This same practice applies to other IR tasks such as QS whose output is often ranked results. When a model is novel, human judges may be used to help build a gold standard. In higher education, this may entail some `red tape'--permission by an institutional review board (IRB), sometimes referred to as ethical review board (ERB), independent ethics committee (IEC), or research ethics board (REB). Regardless of difficulty or obstacles, evaluation is critical to proving and comparing models.

``Thorough evaluations are paramount to assess the effectiveness of research-paper recommender systems, and judge the value of recommendation approaches to be applied in practice or as baseline in other evaluations. The most common evaluation methods are user studies, offline evaluations, and online evaluations. \citep{comparison_2015}'' \cite{meta_recommender_evaluation_2013} collectively have at least 3 works in this area in which they make extremely important points: \cite{comparison_2015, comparative_analysis_2013, meta_recommender_evaluation_2013}. One of their more recent works, entitled \textit{A Comparison of Offline Evaluations, Online Evaluations, and User Studies in the Context of Research-Paper Recommender Systems} \citep{comparison_2015} is well-written and I recommend reading it in its entirety. Herein I will summarize the work as it relates to this domain.

A user study requires the researcher to create a controlled study and have willing testers. This has all the problems associated with these types of studies including sample bias and the challenge of making a realistic test. Recommendations systems can also be tested online by recording metrics including click-through rate (CTR), link-through rate (LTR), and cite-through rate (CiTR). These measurements can be difficult to interpret because it is not a controlled feedback and there is no way to ask the user why they made the choices they did like in a user study, but still seems to be a favored method by \cite{comparison_2015}.

% TODO: from Ariel `(You need a section about online evaluations.) Connect the point of this paragraph a little better. Why are you mentioning this weakness & bias?`
In offline evaluation mode, the system is either compared to some baseline or another metric is for determining goodness of results. Offline modes are more common: \cite{meta_recommender_evaluation_2013} indicate that 69\% of research database recommendation systems were evaluated offline. Only 7\% were evaluated online, while 21\% were user studies, with a remaining 6\% were not evaluated at all \citep{comparison_2015}. ``Offline evaluations typically measure the accuracy of a recommender system based on a groundtruth, but also novelty or serendipity of recommendations can be measured. \citep{Ge:2010:BAE:1864708.1864761}''

\cite{comparative_analysis_2013} found that ``results of offline and online evaluations often contradict each other.'' They propose that this is due to a variety of `human factors' and state that, ``We doubt that researchers will ever be able to reliably predict whether human factors affect the predictive power of offline evaluations.'' This leads them to conclude that unless one is working with the assumption that the ground truth (or computer's results) is inherently better than human intuition and biases, offline testing may not be ideal. As \cite{Ge:2010:BAE:1864708.1864761} discuss, ``high-quality recommendations need to be fitting their intended purpose and the actors behind this purpose are the ultimate judges of the quality of recommendations.'' In other words, the tests used to evaluate effectiveness ought to be based upon the ultimate goal of the project.  

When ordered sets constitute the output, which is the case for search and discovery-based methods, \textit{precision} and \textit{recall} are two commonly trusted metrics. A way to balance them is to compute the \textit{F\textsubscript{1} score} or \textit{f-measure}, which is simply the harmonic mean of the two. It is important to note that these metrics only work when a gold standard exists, i.e. when the best results are known a-priori for some test portion of the dataset. Metrics which do not require a gold standard include \textit{catalog coverage}. The \textit{serendipity} metric requires at least a baseline set of recommendations \citep{Ge:2010:BAE:1864708.1864761}.

The \textit{catalog coverage} metric shows how good a system is at providing results throughout the corpus rather than favoring certain documents. \textit{Catalog coverage} does not require any gold standard or baseline. It is an intrinsic metric. \cite{Ge:2010:BAE:1864708.1864761} note ``Catalog coverage can be a particularly valuable measure for systems that recommend lists of items. \citeyearpar{Ge:2010:BAE:1864708.1864761}'' \textit{Serendipity} measures how good a system is at providing results that are \textit{surprising} rather than \textit{obvious} \citep{Ge:2010:BAE:1864708.1864761}. \textit{Serendipity} requires a baseline to exist; it is an extrinsic metric.

Some or all of these metrics may all be important to any given user. For a person new to a corpus, they may want coverage while for a more experienced user may desire to find serendipitous (unexpected) results. For music recommendation, a system called Auralist was developed by \citeauthor{zhang2012auralist}. As \citeauthor{zhang2012auralist} says, ``An ideal recommendation system should mimic the actions of a trusted friend or expert, producing a personalised collection of recommendations that balance between the desired goals of accuracy, diversity, novelty and serendipity...[Auralist] attempts to balance and improve all four factors simultaneously. \citeyear{zhang2012auralist}''


% nDCP requires a baseline.
When two recommender systems perform perform similarly, they will have similar output. One can measure similarity using formulas in the \textit{nDCG} family \citep{Wang:2006:TOT:1150402.1150450}. This can lend credibility to either system if one tends to provide output that is similar to the already-accepted system. However, this is an extrinsic metric; like \textit{serendipity}, it requires the use of a \textit{common} baseline. Two systems cannot be compared to each other without the presence of a third. If they use each other as proxy as the third, the overall nDCG value for each system ends up being the equal.
