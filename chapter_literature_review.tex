\chapter{Review of Literature}

\section{Algorithms, Models, and Metric Formulas}
%\subsection{A Brief Background of Consumption Large Data}

In our information-based society, researchers want to create meaningful connections and derive useful data or make meaningful connections from textual corpora. Some applications include search query, recommender systems, and summarization \citep{Mani01summarizationevaluation:2001}.
%Search queries are common and many users are familiar with their use.
Search via query is not without its drawbacks. It often requires that a user have at least some knowledge of the corpus being queried. This includes some knowledge of pertinent vocabulary. For people who are new to a body of data
%such as new church lay members accessing the \textit{CLDSGCT},
this is not an entirely reasonable expectation.

A use-case scenario demonstrating this is included in the appendix. For example, a person might be able to locate a handful documents of interest, but finding more can quickly become an increasingly difficult process of sifting through lower and lower-ranked search results, unless the user resorts to more search queries.

This is where recommender systems can help. Given some starting document, a system can find similar documents and characterize them as recommended/similar.
%Presenting recommendations in this way is a use of the long-tail phenomenon.
 Recommender systems typically involve processing phases: first, measurable features are discovered and assigned to items in the dataset and secondly, an algorithm matches items of similar features to create recommendations. When personalized data such as ratings are available, those may be leveraged to provide personalized recommendations.

Often the same algorithms that are used in parts of recommender systems also serve in search queries \citep{}. While many algorithms have been implemented in this area, I will focus on the following in this chapter: TF-IDF in search and recommendation, LDA in recommendation, k-NN, (Collapsed) Gibbs Sampling, Variational Inference, algorithms with provable guarantees, etc..

TF-IDF is a bag-of-words model of term weights stored in matrices. It is short for term frequency inverse document frequency.  It is an accessible model because it is well-known, easy to understand, and included in open-source products such as Lucene \citep{McCandless:2010:LAS:1893016}, Lucene Solr \citep{apache_solr_beginners_guide_2013,apache_solr_enterprise_search_server_2015}, and NatualNode’s natural (\url{https://github.com/NaturalNode/natural}).  It can be used in both recommender systems and search systems, which means that the overall code can be elegant when both search and recommendation systems end up using the same code base--a desirable characteristic for time/money-constrained systems.

Sp\"{a}rck Jones wrote the seminal paper on TF-IDF in \citeyear{sparck1972statistical} \citep{sparck1972statistical}
%It is highly cited. Despite its age, the paper continues to be highly cited and was republished as late as 2004
Sp\"{a}rck’s work inspired other computer scientists such as Stephen Robertson to conduct research on TF-IDF. It has been used quite a bit in the field of search query. Karen Sp\"{a}rck Jones worked with Stephen Robertson to create Okapi BM25 and later improved versions, a set of functions used in document retrieval that uses TF-IDF principles. Later, Stephen Robertson worked to develop the Bing search engine. Open-source tools such as Lucene \citep{McCandless:2010:LAS:1893016}, Lucene Luke \citep{lucene:luke}, AntConc \citep{anthony_2013} have been created as a result of widespread usage of TF-IDF. LDS SCI uses Lucene Luke and LDS.org search results are typically so similar that it would appear that it also uses TF-IDF behind-the-scenes--or something that produces the same results!

Besides using raw frequency to compute the TF scores of the TF-IDF model, \citet{manning_raghavan_2008_scoring} mentions three other ways to compute, namely (1) Boolean ``frequencies'', (2) logarithmically scaled, and (3) augmented frequency. These all affect the algorithm by affecting the built-in bias. They are approaches to the problem of how to avoid giving excess weight to terms found in longer documents. Long documents have a better chance of having key words more frequently, but wordiness doesn’t always correspond with additional information.  Similarly, the IDF portion can be calculated in a variety of ways. Variations of IDF such as smoothing or a probabilistic approach affect the bias of how important any given word is in a corpora. All these variants can pose a challenge for the researcher as he/she must determine the correct biases for the specific project. Another notable way to compute document similarity for search is be to use word correlation factors \citep{won2007using}.

TF-IDF weighting is inspired by Zipf's law which states that some words are used many times while the majority of words will occur less often. The weight, or value, of a word is in inversely proportional to the number of occurrences, hence IDF, inverse document frequency. On this Sp\"{a}rck stated, ``The appropriate way of doing [weighting] is suggested by the term distribution curve for the vocabulary, which has the familiar Zipf shape.'' \citep{Wu:2008:Interpreting_tf_idf_term_weights} similarly said ``the term-frequency factor was originally thought to be indicative of document topic [Luhn 1958], and the inverse document-frequency (IDF) is reasoned [Sp\"arck Jones 1972] on the basis of Zipf law.'' It is important to note that not all IDF score metrics attempt to respect Zipf's law. One such example of this is the unary IDF scoring, which simply assigns a value of 1 to all IDF scores.

TF-IDF can also be used in the creation of a recommender system with all the same bias adjustments mentioned previously. Each document has a corresponding TF-IDF vector which is a structured abstraction. These structures can be compared. Researchers can employ a variety of algorithms to compare the vectors to make meaningful connections. %For example, \citeauthor{Meteren_usingcontent_based} used kNN with a cosine metric and published their findings \citep{Meteren_usingcontent_based}. k-NN will be discussed in further detail later in this literature review.
Where more than 1 textual metadata exists for each document, each metadata can be considered separately, then weighted appropriately as a zone \citep{manning_raghavan_2008_scoring}. Examples of this would be textual titles and textual content. Although they are separate, they may both be used for matching purposes. \citet{Wu:2008:Interpreting_tf_idf_term_weights} provide a novel use of TF-IDF by creating both local relevance that ``only applies to a specific document location, and [non-local], common, type is the ``document-wide'' relevance that applies to the entire document. The model combines the local relevance for every location of a document by the document-wide relevance decision of the document.''

Overall, TF-IDF is considered to be a powerful tool for informational retrieval by experts in the field. Robertson \citeyearpar{understanding_idf_2004} said, ``The class of weighting schemes known generically as TF*IDF, which involve multiplying the IDF measure (possibly one of a number of variants) by a TF measure (again possibly one of a number of variants, not just the raw count) have proved extraordinarily robust and difficult to beat, even by much more carefully worked out models and theories.'' A more notable example includes Netflix’s use of context \citep{Bell:2007:lessons_from_the_netflix_prize}.

Researchers have continued to work on ways to improve text corpora modeling. A significant model was introduced by Blei and associates called Latent Dirichlet Allocation (LDA) in their paper published in 2003 \citep{Blei:2003:LDA:944919.944937}. They asserted that LDA could overcome weakness in previous models. They state that TF-IDF ``provides a relatively small amount of reduction in description length and reveals little in the way of inter- or intra-document statistical structure.'' Pre-dating LDA, the models used by researched were LSI and then pLSI. \citeauthor{Blei:2003:LDA:944919.944937} explain that ``pLSI is incomplete in that it provides no probabilistic model at the level of documents. In pLSI, each document is represented as a list of numbers (the mixing proportions for topics), and there is no generative probabilistic model for these numbers. This leads to several problems: (1) the number of parameters in the model grows linearly with the size of the corpus, which leads to serious problems with overfitting, and (2) it is not clear how to assign probability to a document outside of the training set.'' LDA is a generative probabilistic model, so in these respects, it is an improvement over pLSI.
