\chapter{Literature Review}

\section{Algorithms, Models, and Metric Formulas}
%\subsection{A Brief Background of Consumption Large Data}

Many people have discussed and done work to solve the dilemma of how to create meaningful connections and derive useful data or make meaningful connections from very textual corpuses. Some things done in the past include search query, recommender systems, and even summarization \citep{Mani01summarizationevaluation:2001}. Search queries are common and many users are familiar with their use. However, search via query is not without its drawbacks. It often requires that a user have at least some knowledge of the corpus it is being used on. This includes vocabulary. For people who are new to a body of data such as new church lay members accessing the \textit{CLDSGCT}, this is not a fully viable approach. A use-case scenario demonstrating this is included in the appendix. For example, a person might be able to locate a handful documents of interest, but finding more can quickly become an increasingly difficult process of sifting through lower and lower-ranked search results, unless the user resorts to more search queries--if they even exist. This is where recommender systems can help. Given some starting document, similar documents can be automatically presented as recommended/similar. Presenting recommendations in this way is a use of the long-tail phenomenon. Many times recommender systems are comprised of two parts: first, measurable features are discovered and assigned to items in the dataset and secondly, an algorithm is employed to match items of similar features to create recommendations. When personalized data such as ratings are available, those may be leveraged to provide personalized recommendations. Often these same algorithms are used in parts of recommender systems as well as search queries \citep{}. While there are many algorithms that have been leveraged in this area, I will focus on the following in this literature review: TF-IDF in search and recommendation, LDA in recommendation, k-NN, (Collapsed) Gibbs Sampling, Variational Inference, algorithms with provable guarantees, etc..

TF-IDF is a bag-of-words model of term weights stored in matrices. It is short for term frequency inverse document frequency.  It is an accessible model because it is well-known, easy to understand, and included in open source products such as Lucene \citep{McCandless:2010:LAS:1893016}, Lucene Solr \citep{apache_solr_beginners_guide_2013,apache_solr_enterprise_search_server_2015}, and NatualNode’s natural (\url{https://github.com/NaturalNode/natural}).  It can be used in both recommender systems and search systems, which means that the overall code can be elegant when both search and recommendation systems end up using the same code base--a desirable characteristic for time/money-constrained systems.

The seminal paper in the area of TF-IDF, which is highly cited, is A statistical interpretation of term specificity and its application in retrieval \citep{sparck1972statistical}. Despite its age, the paper continues to be highly cited and was republished as late as 2004 (TODO: cite with `see ...' using Journal of Documentation Volume 60 Number 5 2004 pp. 493-502). Spärck’s work inspired other computer scientists such as Stephen Robertson to conduct research on TF-IDF. It has been used quite a bit in the field of search query. Karen Spärck Jones worked with Stephen Robertson to create Okapi BM25 and later improved versions, a set of functions used in document retrieval that uses TD-IDF principles. Later, Stephen Robertson worked to develop the Bing search engine. Open source tools such as Lucene \citep{McCandless:2010:LAS:1893016}, Lucene Luke \citep{lucene:luke}, AntConc \citep{anthony_2013} have been created as a result of widespread usage of TF-IDF. LDS SCI uses Lucene Luke and LDS.org search results are typically so similar that it would appear that it also uses TF-IDF behind-the-scenes--or something that produces the same results!

Besides using raw frequency to compute the TF scores of the TF-IDF model, \citet{manning_raghavan_2008_scoring} mentions 3 other ways to compute, namely (1) Boolean ``frequencies'', (2) logarithmically scaled, and (3) augmented frequency. These all affect the algorithm by affecting the built-in bias. They are approaches to the problem of how to avoid giving excess weight to terms found in longer documents. Long documents have a better chance of having key words more frequently, but wordiness doesn’t always correspond with additional information.  Similarly, the IDF portion can be calculated in a variety of ways. Variations of IDF such as smoothing or a probabilistic approach affect the bias of how important any given word is in a corpora. All these variants can pose a challenge for the researcher as he/she must determine the correct biases for the specific project. Another notable way to compute document similarity for search is be to use word correlation factors \citep{won2007using}.

TF-IDF weighting is inspired by Zipf’s law which states that some words are used many times while the majority of words will occur less often. The weight, or value, of a word is in inversely proportional to the number of occurrences, hence IDF, inverse document frequency. On this Spärck stated, ``The appropriate way of doing [weighting] is suggested by the term distribution curve for the vocabulary, which has the familiar Zipf shape.'' \citep{Wu:2008:Interpreting_tf_idf_term_weights} similarly said ``the term-frequency factor was originally thought to be indicative of document topic [Luhn 1958], and the inverse document-frequency (IDF) is reasoned [Spa ̈rck Jones 1972] on the basis of Zipf law.’’. It is important to note that not all IDF score metrics attempt to respect Zipf’s law. One such example of this is the unary IDF scoring, which simply assigns a value of 1 to all IDF scores.

TF-IDF can also be used in the creation of a recommender system with all the same bias adjustments mentioned previously. Each document has a corresponding TF-IDF vector which is a structured abstraction. These structures can be compared. Researchers can employ a variety of algorithms to compare the vectors to make meaningful connections. For example, \citeauthor{Meteren_usingcontent_based} used kNN with a cosine metric and published their findings \citep{Meteren_usingcontent_based}. k-NN will be discussed in further detail later in this literature review. Where more than 1 textual metadata exists for each document, each metadata can be considered separately, then weighted appropriately as a zone \citep{manning_raghavan_2008_scoring}. Examples of this would be textual titles and textual content. Although they are separate, they may both be used for matching purposes. \citet{Wu:2008:Interpreting_tf_idf_term_weights} provide a novel use of TF-IDF by creating both local relevance that ``only applies to a specific document location, and [non-local], common, type is the ``document-wide'' relevance that applies to the entire document. The model combines the local relevance for every location of a document by the document-wide relevance decision of the document.’’

Overall, TD-IDF is considered to be a powerful tool for informational retrieval by experts in the field. Robertson \citeyearpar{understanding_idf_2004} said, ``The class of weighting schemes known generically as TF*IDF, which involve multiplying the IDF measure (possibly one of a number of variants) by a TF measure (again possibly one of a number of variants, not just the raw count) have proved extraordinarily robust and difficult to beat, even by much more carefully worked out models and theories.'' A more notable example includes Netflix’s use of context \citep{Bell:2007:lessons_from_the_netflix_prize}.
